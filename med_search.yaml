---
Lasso:
    alpha: [0.00001, 0.00002, 0.00003, 0.00004, 0.00005, 0.0001, 0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009, 0.01, 0.02, 0.1]

ElasticNet:
    alpha: [0.00001, 0.00002, 0.00003, 0.00004, 0.00005, 0.0001, 0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009, 0.01, 0.02, 0.1]

Ridge:
    alpha: [0.00001, 0.00002, 0.00003, 0.00004, 0.00005, 0.0001, 0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009, 0.01, 0.02, 0.1]

PLS:
    n_components: [2, 3, 4, 5, 6, 7, 8, 9, 10]

KNN:
    n_neighbors: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

DecisionTree:
    parameters:
        max_depth: [1, 2, 3, 4, 5, 6]
        min_samples_split: [2, 3, 4, 5]
        max_features: [5, 10, 15]
    n_iter: 200

RandomForest:
    parameters:
        n_estimators: [2, 3, 4, 5, 6, 7, 8, 10, 15, 20, 25, 50, 60, 75, 80, 100] #[int(x) for x in np.linspace(start = 30, stop = 200, num = 10)] # Number of trees in random forest
        max_features: ['auto', 'sqrt'] # Number of features to consider at every split
        max_depth: [2, 3, 4, 5, 10, 20, 50, 60, 70, 80, 90, 100] #[int(x) for x in np.linspace(10, 110, num = 11)] # Maximum number of levels in tree
        min_samples_split: [2, 3, 4, 5, 6, 7, 8, 9, 10] # Minimum number of samples required to split a node
        min_samples_leaf: [1, 2, 3, 4, 5]  # Minimum number of samples required at each leaf node
        bootstrap: [True, False] # Method of selecting samples for training each tree
    n_iter: 200

AdaBoost:
    parameters:
        n_estimators: [10, 20, 30, 40, 50, 60, 75, 100, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250] #[int(x) for x in np.linspace(start = 20, stop = 100, num = 5)] # Number of decision trees as weak learner
        learning_rate: [0.001, 0.005, 0.01, 0.02, 0.1, 1, 2, 3, 4, 5, 6, 10, 15, 20, 30]
        loss: ["linear", 'square', 'exponential']
    n_iter: 200

TPOT:
    generations: 5
    population_size: 50
    verbosity: 2
    max_time_mins: 20
    n_jobs: 2
