---
Lasso: 
    alpha: [0.0001, 0.001, 0.005] #, 0.006, 0.007, 0.008, 0.009, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 1, 10]
    
ElasticNet: 
    alpha: [0.0001, 0.001, 0.005, 0.006, 0.007, 0.008, 0.009, 0.01, 1, 10] #np.arange(10, 200, 10)
    
KNN:
    n_neighbors: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    
DecisionTree:
    parameters:
        max_depth: [1, 2, 3, 4, 5, 6]
        min_samples_split: [2, 3, 4, 5]
        max_features: [5, 10, 15]
    n_iter: 10
    
RandomForest:
    parameters: 
        n_estimators: [5, 10, 15, 20, 25, 50, 60, 75, 80, 100] #[int(x) for x in np.linspace(start = 30, stop = 200, num = 10)] # Number of trees in random forest
        max_features: ['auto', 'sqrt'] # Number of features to consider at every split
        max_depth: [0, 10, 20, 50, 90, 100] #[int(x) for x in np.linspace(10, 110, num = 11)] # Maximum number of levels in tree
        min_samples_split: [2, 3, 4, 5, 6, 7, 8, 9, 10] # Minimum number of samples required to split a node
        min_samples_leaf: [1, 2, 3, 4, 5]  # Minimum number of samples required at each leaf node
        bootstrap: [True, False] # Method of selecting samples for training each tree
    n_iter: 200
    
AdaBoost:
    parameters:
        n_estimators: [5, 10, 15, 20, 25, 50, 60, 65, 70, 100, 150,  200, 250] #[int(x) for x in np.linspace(start = 20, stop = 100, num = 5)] # Number of decision trees as weak learner
        learning_rate: [0.01, 0.1, 1, 2]
        loss: ["linear", 'square', 'exponential'] 
    n_iter: 200
    
TPOT: 
    generations: 5
    population_size: 50
    verbosity: 2
    max_time_mins: 20
    n_jobs: 2