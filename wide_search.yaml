---
Lasso: 
    alpha: [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 20, 100, 150, 200, 225, 250, 275, 300, 325, 350] #np.arange(0.01, 30.01, 1)

ElasticNet:
    alpha: [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 1,  2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 20, 100, 150, 200, 225, 250, 275, 300, 325, 350] #np.arange(10, 200, 10)

RandomForest:
    parameters: 
        n_estimators: [25, 30, 40, 50, 60, 75, 80, 90, 100] #[int(x) for x in np.linspace(start = 30, stop = 200, num = 10)] # Number of trees in random forest
        max_features: ['auto', 'sqrt'] # Number of features to consider at every split
        max_depth: [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100] #[int(x) for x in np.linspace(10, 110, num = 11)] # Maximum number of levels in tree
        min_samples_split: [2, 3, 4, 5, 6, 7, 8, 9, 10] # Minimum number of samples required to split a node
        min_samples_leaf: [1, 2, 3, 4, 5, 6, 7, 8]  # Minimum number of samples required at each leaf node
        bootstrap: [True, False] # Method of selecting samples for training each tree
    n_iter: 300
    
AdaBoost:
    parameters:
        n_estimators: [50, 55, 60, 65, 70, 80, 90, 100, 120, 130, 140, 150, 160, 170, 180, 190, 195, 200, 205, 210, 250] #[int(x) for x in np.linspace(start = 20, stop = 100, num = 5)] # Number of decision trees as weak learner
        learning_rate: [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 2, 3]
        loss: ["linear", 'square', 'exponential'] 
    n_iter: 300
    
# AdaBoost:
#     parameters:
#         n_estimators: [5, 10, 20, 30, 35, 40, 45, 50, 51, 52, 53, 54, 55, 60] #[int(x) for x in np.linspace(start = 20, stop = 100, num = 5)] # Number of decision trees as weak learner
#         learning_rate: [0.05, 0.1, 0.15, 0.2]
#         loss: ['exponential'] 
#     n_iter: 50
    
TPOT: 
    generations: 15
    population_size: 50
    verbosity: 2
    max_time_mins: 20
    n_jobs: 2
